---
title: "612 project 4"
author: "Albina Gallyavova"
date: "4/13/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    fig_height: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r libraries, echo=FALSE}
library(knitr)
library(recommenderlab)
library(tidyverse)
library(DT)
data(Jester5k)
```

## Data  
For this exercise, we will switch from MovieLens dataset to Jester5k.   
```{r}
## get some information
summary(dimnames(Jester5k))
summary(rowCounts(Jester5k)) ## number of ratings per user
summary(colCounts(Jester5k)) ## number of ratings per item
summary(colMeans(Jester5k)) ## average item rating
## histogram of ratings
hist(getRatings(Jester5k), breaks="FD")
image(Jester5k[1:10,1:10])

# users who rated at least 50 jokes and jokes that have been rated at least 100 times
ratings <- Jester5k[rowCounts(Jester5k) > 50, colCounts(Jester5k) > 100]
```

## UBCF vs SVD
As before, we will split the data into train and test and train UBCF and SVD models.    

### Train and test   
```{r}
set.seed(1)
scheme <- ratings %>% 
  evaluationScheme(method = "cross",
                   k=3, #number of folds/times to run the evaluation 
                   train  = 0.8, 
                   given  = 15, # items to use to generate recommendations
                   goodRating = 0 # threshold at which ratings are considered good for evaluation
                   )
train <- getData(scheme, "train") # training set
test <- getData(scheme, "known") # test set with the items used to build recommendations
unknown <- getData(scheme, "unknown") # test set with the items used to test recommendations
```

### Model 1 - UBCF, nn=5, Pearson similarity   
```{r}
# Train a user-based collaborative filtering recommender using a small training set.
ubcf_model_top5 <- Recommender(data=train,method="UBCF", parameter = list(nn=15,method='Pearson'))
ubcf_model_top5

# Create top-N recommendations for new users (users in test set)
ubcf_pred_top5 <- predict(object = ubcf_model_top5, newdata = test, n = 5, type="ratings") # type = 'ratings'
as(predict(ubcf_model_top5, test[101:102],n=5), "list")
```

### Model 2 - SVD  
```{r}
# Train SVD recommender using a small training set
svd_model_top5 <- Recommender(data=train,method="SVD", parameter = NULL)
svd_model_top5
# Create top-N recommendations for new users (users in test set)
svd_pred_top5 <- predict(object = svd_model_top5, newdata = test, n = 5, type="ratings")
as(predict(svd_model_top5, test[101:102],n=5), "list")

```



## Evaluation of ratings  
In addition to UBCF and SVD, we'll compare recommendations generated by `POPULAR` and `RANDOM` methods.  
```{r}
# https://www.rdocumentation.org/packages/recommenderlab/versions/0.2-5/topics/evaluationScheme

algorithms <- list(
  "UBCF_pearson15" = list(name  = "UBCF", param = list( method = "Pearson", nn=15)),
  "SVD" = list(name = "SVD", param = NULL),
  "random items" = list(name="RANDOM", param=NULL),
  "popular items" = list(name="POPULAR", param=NULL))

# https://www.rdocumentation.org/packages/recommenderlab/versions/0.2-5/topics/evaluate
results <- evaluate(scheme, 
                    algorithms, 
                    type = "ratings", # evaluate "topNList" or "ratings"
                    n = c(1, 3, 5, 10,20) # N (number of recommendations) of the top-N lists generated
                    )

avg(results)
plot(results, annotate=TRUE) 
title('Accuracy metrics')
```

## Evaluation of recommendations

```{r}
results <- evaluate(scheme, 
                    algorithms, 
                   # type = "topNList", # evaluate "topNList" or "ratings"
                    n = c(1, 3, 5, 10) # N (number of recommendations) of the top-N lists generated
                    )
plot(results, annotate = TRUE, main = "ROC Curve")

# Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.
plot(results, "prec/rec", annotate = TRUE, main = "Precision-Recall")
```

As expected, `POPULAR` method performs the best in terms of accuracy as it merely recommends the most popular items (jokes) that a user would have end up liking anyhow. Randomly generated items on the other hand, have greatest RMSE and represent the jokes the user might have rated lower.  
We can make use of `HybridRecommender()` function in recommenderlab to combine `POPULAR` and `RANDOM` methods for increased diversity in recommendations. Including random component might also increase serendipitous recommendations. We can also increase the size of recommendations from 5 to 10.  

```{r}
# https://cran.r-project.org/web/packages/recommenderlab/recommenderlab.pdf
hybrid <- HybridRecommender(
    Recommender(train, method = "POPULAR"),
    Recommender(train, method = "RANDOM"),
    weights = c(.5, .5)
)
hybrid
hybrid_pred_top5 <- predict(object = hybrid, newdata = test, n = 10, type="ratings")
as(predict(hybrid, test[101:102]), "list")
```

Comparing 3 algorithms side-by-side, we see that algorithm including random recommendations results in lowest accuracy measured by RMSE.  
```{r}
error <- rbind(
  UBCF = calcPredictionAccuracy(ubcf_pred_top5, unknown)[1:3],
  SVD = calcPredictionAccuracy(svd_pred_top5, unknown)[1:3],
  Hybrid = calcPredictionAccuracy(hybrid_pred_top5, unknown)
  )
error
```


## Conclusion   
There is no simple way to measure user satisfaction with recommendations with offline evaluation, however, there are a couple of methods that could be applied in online evaluation process. One way is to explicitly ask users through a survey (or similar methods) how did they like recommendations provided to them which could allow direct feedback to be fed back into training process. If such functionality is not available, retailers have been know to measure user acceptance by observing click-through rate ratio (number of recommendations that were clicked on), although not ideal as prone to errors, this method provides a better understanding of users' interest and could further enhance recommendations.  
